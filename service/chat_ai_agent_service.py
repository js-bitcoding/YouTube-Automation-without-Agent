# import datetime
# from langchain.tools import Tool
# from sqlalchemy.orm import Session
# from database.db_connection import get_db
# from langchain_community.llms import Ollama
# from fastapi import APIRouter, Depends, HTTPException
# from langchain.agents import initialize_agent, AgentType
# from langchain.memory import ConversationBufferMemory
# from fastapi import APIRouter, Depends, HTTPException
# from database.models import  Group, Document, YouTubeVideo,ChatConversation,ChatHistory,User

# # Initialize the Ollama LLM
# llm = Ollama(model="llama3.2:1b")

# # Setup agent memory
# memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# # Define the tools that will be used by the agent
# def fetch_group_data(group_ids: list, db: Session):
#     """
#     Returns both the formatted string for the LLM and metadata (tone/style info).
#     """
#     formatted_sections = []
#     tone_set = set()
#     style_set = set()

#     for group_id in group_ids:
#         group = db.query(Group).filter(Group.id == group_id).first()
#         if not group:
#             continue

#         section = [f"\n Group: {group.name or 'Unnamed Group'} (ID: {group.id})"]

#         documents = db.query(Document).filter(Document.group_id == group_id).all()
#         for doc in documents:
#             summary = doc.content[:300].strip().replace("\n", " ")
#             section.append(f"Document: {doc.filename}\n Summary: {summary}...\n")

#         videos = db.query(YouTubeVideo).filter(YouTubeVideo.group_id == group_id).all()
#         for video in videos:
#             summary = video.transcript[:300].strip().replace("\n", " ")
#             tone = video.tone or "Unknown"
#             style = video.style or "Unknown"

#             tone_set.add(tone.lower())
#             style_set.add(style.lower())

#             section.append(
#                 f"Video: {video.url}\n Transcript Snippet: {summary}...\n"
#                 f"Tone: {tone.capitalize()},Style: {style.capitalize()}\n"
#             )

#         if len(section) > 1:
#             formatted_sections.append("\n".join(section))

#     return {
#         "formatted": "\n\n".join(formatted_sections),
#         "tones": list(tone_set),
#         "styles": list(style_set)
#     }



# # Define the tool for the agent
# def generate_response_from_prompt_and_data(group_data: str, user_prompt: str):
#     """
#     This tool takes the group data (either document content or video transcript) and the user prompt,
#     and returns a response generated by the LLM.
#     """
#     prompt = f"Here is some content:\n{group_data}\n\nNow, answer the following prompt:\n{user_prompt}"
#     response = llm(prompt)
#     return response

# # Agent setup with the tools and memory
# # title_tool = Tool(
# #     name="ContentBasedAgent",
# #     func=generate_response_from_prompt_and_data,
# #     description="Generates a response based on content from documents and YouTube videos based on the group IDs provided by the user."
# # )

# # agent = initialize_agent(
# #     tools=[title_tool],
# #     llm=llm,
# #     agent=AgentType.OPENAI_FUNCTIONS,
# #     verbose=True,
# #     memory=memory,
# #     handle_parsing_errors=True
# # )

# def generate_response_for_conversation(conversation_id: int, user_prompt: str, db: Session, current_user: User):
#     # Get the conversation
#     conversation = db.query(ChatConversation).filter(
#         ChatConversation.id == conversation_id,
#         ChatConversation.is_deleted == False
#     ).first()
#     if not conversation:
#         raise HTTPException(status_code=404, detail="ChatConversation not found")

#     # Get associated group data
#     chat_session = conversation.session
#     group_ids = [group.id for group in chat_session.groups]
#     group_info = fetch_group_data(group_ids, db)
#     if not group_info["formatted"]:
#         raise HTTPException(status_code=404, detail="No content available for this chat")

#     # Get chat history
#     history_records = db.query(ChatHistory).filter(
#         ChatHistory.chat_conversation_id == conversation.id,
#         ChatHistory.is_deleted == False
#     ).order_by(ChatHistory.created_at.asc()).all()

#     history_prompt = ""
#     for record in history_records:
#         history_prompt += f"User: {record.query}\n"
#         history_prompt += f"Assistant: {record.response}\n"

#     # Final prompt
#     full_prompt = f"""
# You are a helpful assistant having an ongoing conversation with a user.

# The user is referring to content from multiple sources, each with different tone and style. Please synthesize and respond accordingly.
# {group_info["formatted"]}

# Conversation so far:
# {history_prompt}

# User: {user_prompt}
# Assistant:
# """

#     # Generate response
#     try:
#         response = llm(full_prompt)
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"LLM error: {str(e)}")

#     # Save chat
#     new_chat = ChatHistory(
#         query=user_prompt.strip(),
#         response=response.strip(),
#         context=group_info,
#         chat_conversation_id=conversation.id,
#         user_id=current_user.id
#     )
#     db.add(new_chat)
#     db.commit()

#     # Return full response
#     return {
#         "response": response.strip(),
#         "conversation_id": conversation_id,
#         "user_message": user_prompt.strip(),
#         "assistant_message": response.strip(),
#         "based_on_groups": group_ids,
#         "tone_used": ", ".join([tone.capitalize() for tone in group_info["tones"]]) if group_info["tones"] else "",
#         "style_used": ", ".join([style.capitalize() for style in group_info["styles"]]) if group_info["styles"] else "",
#         "history": [
#             {
#                 "sender": "User",
#                 "message": h.query,
#                 "response": h.response,
#                 "timestamp": h.created_at.isoformat()
#             }
#             for h in history_records if h.query
#         ] + [
#             {
#                 "sender": "User",
#                 "message": user_prompt.strip(),
#                 "response": response.strip(),
#                 "timestamp": datetime.datetime.utcnow().isoformat()
#             }
#         ]
#     }




import datetime
from langchain.schema import Document as LangchainDocument
from langchain.embeddings import OllamaEmbeddings  # Use Ollama embeddings
from langchain.vectorstores import FAISS
from langchain.agents import initialize_agent, AgentType
from langchain.llms import Ollama
from langchain.schema import Document  # LangChain Document
from langchain.vectorstores import FAISS
from langchain.embeddings import OllamaEmbeddings 
from langchain.memory import ConversationBufferMemory
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from database.db_connection import get_db
from database.models import Group, Document, YouTubeVideo, ChatConversation, ChatHistory, User

# Initialize the Ollama LLM
llm = Ollama(model="llama3.2:1b")  # Ollama model

# Initialize Ollama Embeddings
ollama_embeddings = OllamaEmbeddings(model="llama3.2:1b")  # Use Ollama embeddings directly

def fetch_group_data(group_ids: list, db: Session):
    """
    Returns both the formatted string for the LLM and metadata (tone/style info),
    using the full content of documents and video transcripts.
    """
    formatted_sections = []
    tone_set = set()
    style_set = set()

    documents = []
    for group_id in group_ids:
        group = db.query(Group).filter(Group.id == group_id).first()
        if not group:
            continue

        section = [f"\nGroup: {group.name or 'Unnamed Group'} (ID: {group.id})"]

        # Collect documents for embedding
        docs = db.query(Document).filter(Document.group_id == group_id).all()
        for doc in docs:
            content = doc.content.strip().replace("\n", " ")
            section.append(f"Document: {doc.filename}\n Full Content: {content}\n")
            documents.append(doc.content)  # Use full content

        # Collect video data
        videos = db.query(YouTubeVideo).filter(YouTubeVideo.group_id == group_id).all()
        for video in videos:
            transcript = video.transcript.strip().replace("\n", " ")
            tone = video.tone or "Unknown"
            style = video.style or "Unknown"

            tone_set.add(tone.lower())
            style_set.add(style.lower())

            section.append(
                f"Video: {video.url}\n Full Transcript: {transcript}\n"
                f"Tone: {tone.capitalize()}, Style: {style.capitalize()}\n"
            )
        
        if len(section) > 1:
            formatted_sections.append("\n".join(section))
            print(formatted_sections)
    return {
        "formatted": "\n\n".join(formatted_sections),
        "tones": list(tone_set),
        "styles": list(style_set),
        "documents": documents  # Full document content
    }

from langchain.text_splitter import RecursiveCharacterTextSplitter

def initialize_faiss_store(documents: list):
    """
    Initialize FAISS vector store for document retrieval, using chunked documents.
    """
    # Use text splitter to chunk documents
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # Customize chunk size
        chunk_overlap=100  # Optional overlap for better context retention
    )

    # Create LangChain Document objects with chunking
    all_chunks = []
    for doc in documents:
        chunks = splitter.create_documents([doc])  # Returns list of LangchainDocuments
        all_chunks.extend(chunks)
        for i, chunk in enumerate(chunks):
            print(f"Chunk {i+1}: {chunk.page_content}")

    # Create FAISS vector store with chunked documents
    vectorstore = FAISS.from_documents(all_chunks, ollama_embeddings)
    print(f"Vectorstore created with {len(all_chunks)} chunks.")
    return vectorstore

# Define the tool for the agent
def generate_response_from_prompt_and_data(group_data: str, user_prompt: str):
    """
    This tool takes the group data (either document content or video transcript) and the user prompt,
    and returns a response generated by the LLM.
    """
    prompt = f"Here is some content:\n{group_data}\n\nNow, answer the following prompt:\n{user_prompt}"
    response = llm(prompt)  # Use Ollama LLM for generating responses
    return response




# Function to generate the response for a conversation
def generate_response_for_conversation(conversation_id: int, user_prompt: str, db: Session, current_user: User):
    # Get the conversation
    conversation = db.query(ChatConversation).filter(
        ChatConversation.id == conversation_id,
        ChatConversation.is_deleted == False
    ).first()
    if not conversation:
        raise HTTPException(status_code=404, detail="ChatConversation not found")

    # Get associated group data
    chat_session = conversation.session
    group_ids = [group.id for group in chat_session.groups]
    group_info = fetch_group_data(group_ids, db)
    if not group_info["formatted"]:
        raise HTTPException(status_code=404, detail="No content available for this chat")

    # Initialize FAISS vector store for document retrieval
    vectorstore = initialize_faiss_store(group_info["documents"])

    # Get chat history
    history_records = db.query(ChatHistory).filter(
        ChatHistory.chat_conversation_id == conversation.id,
        ChatHistory.is_deleted == False
    ).order_by(ChatHistory.created_at.asc()).all()

    history_prompt = ""
    for record in history_records:
        history_prompt += f"User: {record.query}\n"
        history_prompt += f"Assistant: {record.response}\n"

    # Use vector store to retrieve relevant document content based on user prompt
    search_results = vectorstore.similarity_search(user_prompt, k=3)  # Adjust k to control the number of retrieved docs

    # Combine the group content (from documents and videos) and retrieval results
    retrieved_data = "\n\n".join([result.page_content for result in search_results])

    full_prompt = f"""
You are a helpful assistant having an ongoing conversation with a user.

The user is referring to content from multiple sources, each with different tone and style. Please synthesize and respond accordingly.
{group_info["formatted"]}

Retrieved content:
{retrieved_data}

Conversation so far:
{history_prompt}

User: {user_prompt}
Assistant:
"""

    # Generate response using the LLM
    try:
        response = llm(full_prompt)  # Use Ollama LLM for generating the final response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM error: {str(e)}")

    # Save chat history
    new_chat = ChatHistory(
        query=user_prompt.strip(),
        response=response.strip(),
        context=group_info,
        chat_conversation_id=conversation.id,
        user_id=current_user.id
    )
    db.add(new_chat)
    db.commit()

    # Return the response with context and history
    return {
        "response": response.strip(),
        "conversation_id": conversation_id,
        "user_message": user_prompt.strip(),
        "assistant_message": response.strip(),
        "based_on_groups": group_ids,
        "tone_used": ", ".join([tone.capitalize() for tone in group_info["tones"]]) if group_info["tones"] else "",
        "style_used": ", ".join([style.capitalize() for style in group_info["styles"]]) if group_info["styles"] else "",
        "history": [
            {
                "sender": "User",
                "message": h.query,
                "response": h.response,
                "timestamp": h.created_at.isoformat()
            }
            for h in history_records if h.query
        ] + [
            {
                "sender": "User",
                "message": user_prompt.strip(),
                "response": response.strip(),
                "timestamp": datetime.datetime.utcnow().isoformat()
            }
        ]
    }
