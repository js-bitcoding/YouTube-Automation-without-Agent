import datetime
from fastapi import HTTPException
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from database.models import Group, Document, YouTubeVideo, ChatConversation, ChatHistory, User,Instruction
from utils.logging_utils import logger

llm = OllamaLLM(model="tinyllama:1.1b")

ollama_embeddings = OllamaEmbeddings(model="llama3.2:1b")

def fetch_group_data(group_ids: list, db: Session):
    """
    Returns both the formatted string for the LLM and metadata (tone/style info),
    using the full content of documents and video transcripts.
    """
    formatted_sections = []
    tone_set = set()
    style_set = set()

    documents = []
    for group_id in group_ids:
        group = db.query(Group).filter(Group.id == group_id).first()
        if not group:
            logger.warning(f"Group with ID {group_id} not found.")
            continue

        section = [f"\nGroup: {group.name or 'Unnamed Group'} (ID: {group.id})"]

        docs = db.query(Document).filter(Document.group_id == group_id).all()
        logger.info(f"Found {len(docs)} documents in Group {group_id}")
        for doc in docs:
            content = doc.content.strip().replace("\n", " ")
            section.append(f"Document: {doc.filename}\n Full Content: {content}\n")
            documents.append(doc.content)

        videos = db.query(YouTubeVideo).filter(YouTubeVideo.group_id == group_id).all()
        logger.info(f"Found {len(videos)} videos in Group {group_id}")
        for video in videos:
            transcript = video.transcript.strip().replace("\n", " ")
            tone = video.tone or "Unknown"
            style = video.style or "Unknown"

            tone_set.add(tone.lower())
            style_set.add(style.lower())

            section.append(
                f"Video: {video.url}\n Full Transcript: {transcript}\n"
                f"Tone: {tone.capitalize()}, Style: {style.capitalize()}\n"
            )
        
        if len(section) > 1:
            formatted_sections.append("\n".join(section))
            logger.debug(f"Formatted section for group {group_id}")

    return{
        "formatted": "\n\n".join(formatted_sections),
        "tones": list(tone_set),
        "styles": list(style_set),
        "documents": documents
    }

def initialize_faiss_store(documents: list):
    """
    Initialize FAISS vector store for document retrieval, using chunked documents.
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100
    )

    all_chunks = []
    for doc in documents:
        chunks = splitter.create_documents([doc])
        all_chunks.extend(chunks)

    vectorstore = FAISS.from_documents(all_chunks, ollama_embeddings)
    return vectorstore, all_chunks

def generate_response_from_prompt_and_data(group_data: str, user_prompt: str):
    """
    This tool takes the group data (either document content or video transcript) and the user prompt,
    and returns a response generated by the LLM.
    """
    prompt = f"Here is some content:\n{group_data}\n\nNow, answer the following prompt:\n{user_prompt}"
    response = llm.invoke(prompt)
    return response

def generate_response_for_conversation(conversation_id: int, user_prompt: str, db: Session, current_user: User):
    conversation = db.query(ChatConversation).filter(
        ChatConversation.id == conversation_id,
        ChatConversation.is_deleted == False
    ).first()
    if not conversation:
        raise HTTPException(status_code=404, detail="ChatConversation not found")

    chat_session = conversation.session
    group_ids = [group.id for group in chat_session.groups]
    group_info = fetch_group_data(group_ids, db)
    if not group_info["formatted"]:
        raise HTTPException(status_code=404, detail="No content available for this chat")

    vectorstore, all_chunks = initialize_faiss_store(group_info["documents"])

    max_chunks = 10
    chunked_text = "\n\n".join([chunk.page_content for chunk in all_chunks[:max_chunks]])

    history_records = db.query(ChatHistory).filter(
        ChatHistory.chat_conversation_id == conversation.id,
        ChatHistory.is_deleted == False
    ).order_by(ChatHistory.created_at.asc()).all()

    instructions = db.query(Instruction).filter(
        Instruction.is_deleted == False,
        Instruction.is_activate == True
    ).all()

    history_prompt = ""
    for record in history_records:
        history_prompt += f"User: {record.query}\n"
        history_prompt += f"Assistant: {record.response}\n"

    search_results = vectorstore.similarity_search(user_prompt, k=3)
    retrieved_data = "\n\n".join([result.page_content for result in search_results])
    instructions_info = "\n".join([f" Instruction: {instr.content}" for instr in instructions])

    full_prompt = f"""
        You are a highly capable and context-aware assistant helping a user with information from documents, videos, and previous conversations. Use the instructions and retrieved knowledge to respond in a helpful, clear, and tone-adaptive manner.

        ##  Active User Instructions:
        {instructions_info or "None provided."}

        ##  Primary Context (Documents & Video Transcripts):
        The following is background information from the userâ€™s selected groups. Use it to understand the broader context, tone, and style. Incorporate relevant information, but do not repeat it verbatim unless directly relevant.

        {group_info["formatted"]}

        These are raw extracted chunks from your documents, chunked for better context understanding.
        {chunked_text}

        ##  Communication Style Guidance:
        The content involves various tones and styles:
        - Tones: {", ".join(group_info["tones"]).capitalize() or "Neutral"}
        - Styles: {", ".join(group_info["styles"]).capitalize() or "Plain"}

        Match your response tone and style to align with these.

        ##  Chat History:
        Maintain conversational continuity. Refer to prior user and assistant messages as needed.

        {history_prompt}

        ## ðŸ“© User Prompt:
        {user_prompt}

        ## ðŸ¤– Assistant Response:
        """

    try:
        response = llm.invoke(full_prompt)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM error: {str(e)}")

    new_chat = ChatHistory(
        query=user_prompt.strip(),
        response=response.strip(),
        context=group_info,
        chat_conversation_id=conversation.id,
        user_id=current_user.id
    )
    db.add(new_chat)
    db.commit()

    return JSONResponse(content={
        "response": response.strip(),
        "conversation_id": conversation_id,
        "user_message": user_prompt.strip(),
        "assistant_message": response.strip(),
        "based_on_groups": group_ids,
        "tone_used": ", ".join([tone.capitalize() for tone in group_info["tones"]]) if group_info["tones"] else "",
        "style_used": ", ".join([style.capitalize() for style in group_info["styles"]]) if group_info["styles"] else "",
        "history": [
            {
                "sender": "User",
                "message": h.query,
                "response": h.response,
                "timestamp": h.created_at.isoformat()
            }
            for h in history_records if h.query
        ] + [
            {
                "sender": "User",
                "message": user_prompt.strip(),
                "response": response.strip(),
                "timestamp": datetime.datetime.utcnow().isoformat()
            }
        ]
    })
